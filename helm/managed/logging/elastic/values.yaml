# Default values for a lightweight Elastic Stack instance which can process HPCC component logs.

##The elasticsearch component can be customized by modifying helm chart values here.
elasticsearch:
  enabled: true
  description: "HPCC Managed Elasticsearch"
  ##See https://github.com/elastic/helm-charts/blob/master/elasticsearch/values.yaml for all available options
  antiAffinity: "soft"  #default is HARD, for minimal systems soft might be necessary
  replicas: 1           #default is 3, for minimal systems 1 replicas should be adequate
  minimumMasterNodes: 1 #default is 2, for minimal systems 1 master node should be adequate
  labels: {"managedby" : "HPCC"}
  clusterHealthCheckParams: "local=true" #local node health status
  #clusterHealthCheckParams: "wait_for_status=yellow&timeout=50s"
  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 0
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5
  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: 5Gi
  #persistence:
    #enabled: true
    #labels:
      # Add default labels for the volumeClaimTemplate fo the StatefulSet
      #enabled: false
    #annotations: {}
  extraEnvs:
    - name: WAIT_ES_READY_IN_SEC
      value: "100"
    #- name: STATUS_COLOR
    #  value: "yellow"
  lifecycle:
    postStart:
      exec:
        command:
          - bash
          - -c
          - |
            PIPELINE_NAME=hpccpipeline
            ES_URL=http://localhost:9200
            ES_CLUSTER_HEALTH_PATH=_cluster/health
            ES_CLUSTER_HEALTH_PARAMS=_cluster/health
            PIPELINE_REQ_PATH=_ingest/pipeline
            [ -z ${WAIT_ES_READY_IN_SEC} ] && WAIT_ES_READY_IN_SEC=100
            END_TIME=$(expr $(date +%s) \+ ${WAIT_ES_READY_IN_SEC})
            ##START_FILE=/tmp/.es_start_file
            ##ls $START_FILE
            while [[ "$(curl -s -o /dev/null -w '%{http_code}\n' $ES_URL)" != "200" ]]; do
              echo "\nES not ready..."
              [ $(date +%s) -ge ${END_TIME} ] && break
              sleep 1
            done
            #curl -X GET "$ES_URL/$ES_CLUSTER_HEALTH_PATH?wait_for_status=${STATUS_COLOR}&timeout=50s&pretty"  --fail --silent --show-error -o /dev/stderr
            curl -X PUT "$ES_URL/$PIPELINE_REQ_PATH/$PIPELINE_NAME/" -H 'Content-Type: application/json' \
              -d'{ "description": "Parses and formats HPCC Systems component log entries", "processors": [{ "grok": { "field": "message", "patterns": ["%{BASE16NUM:hpcc.log.sequence}\\s+%{HPCC_LOG_AUDIENCE:hpcc.log.audience}\\s+%{HPCC_LOG_CLASS:hpcc.log.class}\\s+%{TIMESTAMP_ISO8601:hpcc.log.timestamp}\\s+%{POSINT:hpcc.log.procid}\\s+%{POSINT:hpcc.log.threadid}\\s+%{HPCC_LOG_WUID:hpcc.log.jobid}\\s+%{QUOTEDSTRING:hpcc.log.message}"], "pattern_definitions": { "HPCC_LOG_WUID": "([A-Z][0-9]{8}-[0-9]{6})|(UNK)", "HPCC_LOG_CLASS": "DIS|ERR|WRN|INF|PRO|MET|UNK", "HPCC_LOG_AUDIENCE": "OPR|USR|PRG|AUD|UNK" } } }], "on_failure": [{ "set": { "field": "error.message", "value": "{{ _ingest.on_failure_message }}" } }] }'

  tolerations:
  - key: "kubernetes.azure.com/scalesetpriority"
    operator: "Equal"
    value:  "spot"
    effect: "NoSchedule"
##The filebeat component can be customized by modifying helm chart values here.
filebeat:
  description: "HPCC Managed filebeat"
  tolerations:
  - key: "kubernetes.azure.com/scalesetpriority"
    operator: "Equal"
    value:  "spot"
    effect: "NoSchedule"
  ##See https://github.com/elastic/helm-charts/blob/master/filebeat/values.yaml for all available options
  labels: {"managedby" : "HPCC"}
  ## Allows you to add any config files in /usr/share/filebeat
  ## such as filebeat.yml
  filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: container
        paths:
          - /var/log/containers/esdl-sandbox-*.log
          - /var/log/containers/eclwatch-*.log
          - /var/log/containers/mydali-*.log
          - /var/log/containers/eclqueries-*.log
          - /var/log/containers/sql2ecl-*.log
          - /var/log/containers/eclservices-*.log
          - /var/log/containers/dfuserver-*.log
          - /var/log/containers/eclscheduler-*.log
          - /var/log/containers/hthor-*.log
          - /var/log/containers/myeclccserver-*.log
          - /var/log/containers/roxie-*.log
          - /var/log/containers/sasha-*.log
          - /var/log/containers/thor-*.log
        #exclude_files: ['(myelk-kibana|myelk-filebeat)+(.*).log']
        processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
      #Required if targeting non-default index (filebeat-%{[agent.version]}-%{+yyyy.MM.dd}) such as hpccsystems-%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}, etc.
       #setup.ilm.enabled: false
       #setup.template.overwrite: true
      output.elasticsearch:
        host: '${NODE_NAME}'
        hosts: '${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}'
      #NOTE: Pipeline could be used to provide Log structure and therefore enhance search capabilities of HPCC component log entries
      #      Pipeline must be manually inserted either via Elastic Search API, or Kibana Pipeline ingest UI.
      #      See https://github.com/hpcc-systems/HPCC-Platform/blob/master/helm/managed/logging/elastic/README.md
      #  pipeline: 'hpccpipeline'
      #  index: "hpccsystems-%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}"
      #setup.template.name: hpccsystems
      #setup.template.pattern: hpccsystems-*
      #setup.template.enabled: true

##The kibana component can be customized by modifying helm chart values here.
kibana:
  enabled: true
  description: "HPCC Managed Kibana"
  tolerations:
  - key: "kubernetes.azure.com/scalesetpriority"
    operator: "Equal"
    value:  "spot"
    effect: "NoSchedule"
  ##See https://github.com/elastic/helm-charts/blob/master/kibana/values.yaml for all available options
  labels: {"managedby" : "HPCC"}
  ## Allows you to add any config files in /usr/share/kibana/config/
  ## such as kibana.yml
  #kibanaConfig: {}
  #   kibana.yml: |
  #     key:
  #       nestedkey: value
  service:
    type: "LoadBalancer"
    annotations:
      # This annotation delcares the Azure load balancer for the service as internal rather than internet-visible
      #service.beta.kubernetes.io/azure-load-balancer-internal: "true"
      service.beta.kubernetes.io/azure-load-balancer-internal: "false"

      # Enable appropriate annotation for target cloud provider to ensure Kibana access is internal
      #
      #service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"
      #cloud.google.com/load-balancer-type: "Internal"
      #service.beta.kubernetes.io/aws-load-balancer-internal: "true"
      #service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
